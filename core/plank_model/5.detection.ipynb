{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Detection with the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Drawing helpers\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct the input structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine important landmarks for plank\n",
    "IMPORTANT_LMS = [\n",
    "    \"NOSE\",\n",
    "    \"LEFT_SHOULDER\",\n",
    "    \"RIGHT_SHOULDER\",\n",
    "    \"LEFT_ELBOW\",\n",
    "    \"RIGHT_ELBOW\",\n",
    "    \"LEFT_WRIST\",\n",
    "    \"RIGHT_WRIST\",\n",
    "    \"LEFT_HIP\",\n",
    "    \"RIGHT_HIP\",\n",
    "    \"LEFT_KNEE\",\n",
    "    \"RIGHT_KNEE\",\n",
    "    \"LEFT_ANKLE\",\n",
    "    \"RIGHT_ANKLE\",\n",
    "    \"LEFT_HEEL\",\n",
    "    \"RIGHT_HEEL\",\n",
    "    \"LEFT_FOOT_INDEX\",\n",
    "    \"RIGHT_FOOT_INDEX\",\n",
    "]\n",
    "\n",
    "# Generate all columns of the data frame\n",
    "\n",
    "HEADERS = [\"label\"] # Label column\n",
    "\n",
    "for lm in IMPORTANT_LMS:\n",
    "    HEADERS += [f\"{lm.lower()}_x\", f\"{lm.lower()}_y\", f\"{lm.lower()}_z\", f\"{lm.lower()}_v\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup some important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_important_keypoints(results) -> list:\n",
    "    '''\n",
    "    Extract important keypoints from mediapipe pose detection\n",
    "    '''\n",
    "    landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "    data = []\n",
    "    for lm in IMPORTANT_LMS:\n",
    "        keypoint = landmarks[mp_pose.PoseLandmark[lm].value]\n",
    "        data.append([keypoint.x, keypoint.y, keypoint.z, keypoint.visibility])\n",
    "    \n",
    "    return np.array(data).flatten().tolist()\n",
    "\n",
    "\n",
    "def rescale_frame(frame, percent=50):\n",
    "    '''\n",
    "    Rescale a frame to a certain percentage compare to its original frame\n",
    "    '''\n",
    "    width = int(frame.shape[1] * percent/ 100)\n",
    "    height = int(frame.shape[0] * percent/ 100)\n",
    "    dim = (width, height)\n",
    "    return cv2.resize(frame, dim, interpolation =cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH1 = \"../data/plank/plank_test.mov\"\n",
    "VIDEO_PATH2 = \"../data/plank/plank_test_1.mp4\"\n",
    "VIDEO_PATH3 = \"../data/plank/plank_test_2.mp4\"\n",
    "VIDEO_PATH4 = \"../data/plank/plank_test_3.mp4\"\n",
    "VIDEO_PATH5 = \"../data/plank/plank_test_4.mp4\"\n",
    "VIDEO_TEST = \"../../demo/plank.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Make detection with Scikit learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "with open(\"./model/LR_model.pkl\", \"rb\") as f:\n",
    "    sklearn_model = pickle.load(f)\n",
    "\n",
    "# Dump input scaler\n",
    "with open(\"./model/input_scaler.pkl\", \"rb\") as f2:\n",
    "    input_scaler = pickle.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "current_stage = \"\"\n",
    "prediction_probability_threshold = 0.6\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, image = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Reduce size of a frame\n",
    "        image = rescale_frame(image, 50)\n",
    "        # image = cv2.flip(image, 1)\n",
    "\n",
    "        # Recolor image from BGR to RGB for mediapipe\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        results = pose.process(image)\n",
    "\n",
    "        if not results.pose_landmarks:\n",
    "            print(\"No human found\")\n",
    "            continue\n",
    "\n",
    "        # Recolor image from BGR to RGB for mediapipe\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw landmarks and connections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, mp_drawing.DrawingSpec(color=(244, 117, 66), thickness=2, circle_radius=2), mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=1))\n",
    "\n",
    "        # Make detection\n",
    "        try:\n",
    "            # Extract keypoints from frame for the input\n",
    "            row = extract_important_keypoints(results)\n",
    "            X = pd.DataFrame([row], columns=HEADERS[1:])\n",
    "            X = pd.DataFrame(input_scaler.transform(X))\n",
    "\n",
    "            # Make prediction and its probability\n",
    "            predicted_class = sklearn_model.predict(X)[0]\n",
    "            prediction_probability = sklearn_model.predict_proba(X)[0]\n",
    "            # print(predicted_class, prediction_probability)\n",
    "\n",
    "            # Evaluate model prediction\n",
    "            if predicted_class == \"C\" and prediction_probability[prediction_probability.argmax()] >= prediction_probability_threshold:\n",
    "                current_stage = \"Correct\"\n",
    "            elif predicted_class == \"L\" and prediction_probability[prediction_probability.argmax()] >= prediction_probability_threshold: \n",
    "                current_stage = \"Low back\"\n",
    "            elif predicted_class == \"H\" and prediction_probability[prediction_probability.argmax()] >= prediction_probability_threshold: \n",
    "                current_stage = \"High back\"\n",
    "            else:\n",
    "                current_stage = \"unk\"\n",
    "            \n",
    "            # Visualization\n",
    "            # Status box\n",
    "            cv2.rectangle(image, (0, 0), (250, 60), (245, 117, 16), -1)\n",
    "\n",
    "            # Display class\n",
    "            cv2.putText(image, \"CLASS\", (95, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, current_stage, (90, 40), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Display probability\n",
    "            cv2.putText(image, \"PROB\", (15, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(round(prediction_probability[np.argmax(prediction_probability)], 2)), (10, 40), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        cv2.imshow(\"CV2\", image)\n",
    "        \n",
    "        # Press Q to close cv2 window\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # (Optional)Fix bugs cannot close windows in MacOS (https://stackoverflow.com/questions/6116564/destroywindow-does-not-close-window-on-mac-using-python-and-opencv)\n",
    "    for i in range (1, 5):\n",
    "        cv2.waitKey(1)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Make detection with Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ram://0b15f071-04c9-4b5c-bdb7-b4e68fa9b1dd/variables/variables\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./model/plank_dp.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 3\u001b[0m     deep_learning_model \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fyp\\lib\\site-packages\\keras\\saving\\pickle_utils.py:47\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[1;34m(serialized_model)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mGFile(dest_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     46\u001b[0m                 f\u001b[38;5;241m.\u001b[39mwrite(archive\u001b[38;5;241m.\u001b[39mextractfile(name)\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m---> 47\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43msave_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mrmtree(temp_dir)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fyp\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fyp\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:933\u001b[0m, in \u001b[0;36mload_partial\u001b[1;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[0;32m    930\u001b[0m   loader \u001b[38;5;241m=\u001b[39m Loader(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0;32m    931\u001b[0m                   ckpt_options, options, filters)\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 933\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m    934\u001b[0m       \u001b[38;5;28mstr\u001b[39m(err) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m You may be trying to load on a different device \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    935\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom the computational device. Consider setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    936\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto the io_device such as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/job:localhost\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    938\u001b[0m root \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    939\u001b[0m root\u001b[38;5;241m.\u001b[39mgraph_debug_info \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39madjust_debug_info_func_names(debug_info)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ram://0b15f071-04c9-4b5c-bdb7-b4e68fa9b1dd/variables/variables\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'."
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "with open(\"./model/plank_dp.pkl\", \"rb\") as f:\n",
    "    deep_learning_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(VIDEO_TEST)\n",
    "current_stage = \"\"\n",
    "prediction_probability_threshold = 0.6\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, image = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Reduce size of a frame\n",
    "        image = rescale_frame(image, 50)\n",
    "\n",
    "        # Recolor image from BGR to RGB for mediapipe\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        results = pose.process(image)\n",
    "\n",
    "        if not results.pose_landmarks:\n",
    "            print(\"No human found\")\n",
    "            continue\n",
    "\n",
    "        # Recolor image from BGR to RGB for mediapipe\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw landmarks and connections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, mp_drawing.DrawingSpec(color=(244, 117, 66), thickness=2, circle_radius=2), mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=1))\n",
    "\n",
    "        # Make detection\n",
    "        try:\n",
    "            # Extract keypoints from frame for the input\n",
    "            row = extract_important_keypoints(results)\n",
    "            X = pd.DataFrame([row, ], columns=HEADERS[1:])\n",
    "            X = pd.DataFrame(input_scaler.transform(X))\n",
    "            \n",
    "\n",
    "            # Make prediction and its probability\n",
    "            prediction = deep_learning_model.predict(X)\n",
    "            predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "\n",
    "            prediction_probability = max(prediction.tolist()[0])\n",
    "            # print(X)\n",
    "\n",
    "            # Evaluate model prediction\n",
    "            if predicted_class == 0 and prediction_probability >= prediction_probability_threshold:\n",
    "                current_stage = \"Correct\"\n",
    "            elif predicted_class == 2 and prediction_probability >= prediction_probability_threshold: \n",
    "                current_stage = \"Low back\"\n",
    "            elif predicted_class == 1 and prediction_probability >= prediction_probability_threshold: \n",
    "                current_stage = \"High back\"\n",
    "            else:\n",
    "                current_stage = \"Unknown\"\n",
    "\n",
    "            # Visualization\n",
    "            # Status box\n",
    "            cv2.rectangle(image, (0, 0), (550, 60), (245, 117, 16), -1)\n",
    "\n",
    "            # # Display class\n",
    "            cv2.putText(image, \"DETECTION\", (95, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, current_stage, (90, 40), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # # Display class\n",
    "            cv2.putText(image, \"CLASS\", (350, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(predicted_class), (345, 40), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # # Display probability\n",
    "            cv2.putText(image, \"PROB\", (15, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(round(prediction_probability, 2)), (10, 40), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        cv2.imshow(\"CV2\", image)\n",
    "        \n",
    "        # Press Q to close cv2 window\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # (Optional)Fix bugs cannot close windows in MacOS (https://stackoverflow.com/questions/6116564/destroywindow-does-not-close-window-on-mac-using-python-and-opencv)\n",
    "    for i in range (1, 5):\n",
    "        cv2.waitKey(1)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "9260f401923fb5c4108c543a7d176de9733d378b3752e49535ad7c43c2271b65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
